{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“ˆ RUL Prediction Training (Transformer + Attention)\n",
                "\n",
                "This notebook uses a **Transformer Encoder** for RUL prediction on C-MAPSS.\n",
                "\n",
                "**Improvements over LSTM:**\n",
                "- Self-attention captures long-range dependencies\n",
                "- Parallel processing (faster training)\n",
                "- Better at handling variable operating conditions\n",
                "\n",
                "**Expected RMSE:** FD001 ~12-15, FD002 ~22-26"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from tqdm import tqdm\n",
                "import os\n",
                "import math\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============= UPDATE THIS PATH =============\n",
                "DATA_PATH = '/content/drive/MyDrive/MaintanenceAI/Data/CMaps'\n",
                "SAVE_PATH = '/content/drive/MyDrive/MaintanenceAI/trained_models'\n",
                "# =============================================\n",
                "\n",
                "SUBSETS = ['FD001', 'FD002', 'FD003', 'FD004']\n",
                "\n",
                "# Hyperparameters (optimized for Transformer)\n",
                "SEQUENCE_LENGTH = 80        # Longer context\n",
                "MAX_RUL = 125\n",
                "D_MODEL = 64                # Transformer dimension\n",
                "N_HEADS = 4                 # Attention heads\n",
                "N_LAYERS = 3                # Transformer layers\n",
                "D_FF = 128                  # Feed-forward dimension\n",
                "DROPOUT = 0.1\n",
                "\n",
                "EPOCHS = 100                # More epochs\n",
                "BATCH_SIZE = 64\n",
                "LEARNING_RATE = 1e-4        # Lower LR for Transformer\n",
                "\n",
                "print(f'Training Transformer on: {SUBSETS}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Transformer Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "    \"\"\"Adds positional information to sequence.\"\"\"\n",
                "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
                "        super().__init__()\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        \n",
                "        pe = torch.zeros(max_len, d_model)\n",
                "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
                "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
                "        \n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "        pe = pe.unsqueeze(0)\n",
                "        self.register_buffer('pe', pe)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x + self.pe[:, :x.size(1), :]\n",
                "        return self.dropout(x)\n",
                "\n",
                "\n",
                "class TransformerRUL(nn.Module):\n",
                "    \"\"\"Transformer Encoder for RUL Prediction with Attention.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size, d_model=64, n_heads=4, n_layers=3, d_ff=128, dropout=0.1):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Input projection\n",
                "        self.input_proj = nn.Linear(input_size, d_model)\n",
                "        \n",
                "        # Positional encoding\n",
                "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
                "        \n",
                "        # Transformer encoder\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=d_model,\n",
                "            nhead=n_heads,\n",
                "            dim_feedforward=d_ff,\n",
                "            dropout=dropout,\n",
                "            activation='gelu',\n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
                "        \n",
                "        # Attention pooling (learns which timesteps matter most)\n",
                "        self.attention_weights = nn.Linear(d_model, 1)\n",
                "        \n",
                "        # Output regression\n",
                "        self.regressor = nn.Sequential(\n",
                "            nn.Linear(d_model, 32),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(32, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: [batch, seq_len, features]\n",
                "        \n",
                "        # Project to d_model\n",
                "        x = self.input_proj(x)\n",
                "        \n",
                "        # Add positional encoding\n",
                "        x = self.pos_encoder(x)\n",
                "        \n",
                "        # Transformer encoding\n",
                "        x = self.transformer(x)\n",
                "        \n",
                "        # Attention pooling (weighted average)\n",
                "        attn_scores = torch.softmax(self.attention_weights(x), dim=1)\n",
                "        x = torch.sum(x * attn_scores, dim=1)  # [batch, d_model]\n",
                "        \n",
                "        # Predict RUL\n",
                "        return self.regressor(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "COLUMN_NAMES = ['unit', 'cycle', 'op1', 'op2', 'op3'] + [f'sensor_{i}' for i in range(1, 22)]\n",
                "DROP_SENSORS = ['sensor_1', 'sensor_5', 'sensor_6', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n",
                "\n",
                "def get_feature_columns():\n",
                "    all_sensors = [f'sensor_{i}' for i in range(1, 22)]\n",
                "    feature_cols = [s for s in all_sensors if s not in DROP_SENSORS]\n",
                "    return ['op1', 'op2', 'op3'] + feature_cols\n",
                "\n",
                "def load_data(subset):\n",
                "    train_df = pd.read_csv(f'{DATA_PATH}/train_{subset}.txt', sep=r'\\s+', header=None, names=COLUMN_NAMES)\n",
                "    test_df = pd.read_csv(f'{DATA_PATH}/test_{subset}.txt', sep=r'\\s+', header=None, names=COLUMN_NAMES)\n",
                "    rul_true = pd.read_csv(f'{DATA_PATH}/RUL_{subset}.txt', header=None).values.flatten()\n",
                "    return train_df, test_df, rul_true\n",
                "\n",
                "def compute_rul(df, max_rul=125):\n",
                "    df = df.copy()\n",
                "    max_cycles = df.groupby('unit')['cycle'].max()\n",
                "    df['RUL'] = df.apply(lambda row: max_cycles[row['unit']] - row['cycle'], axis=1)\n",
                "    df['RUL'] = df['RUL'].clip(upper=max_rul)\n",
                "    return df\n",
                "\n",
                "def create_sequences(df, seq_len, feature_cols):\n",
                "    sequences, targets = [], []\n",
                "    for unit_id in df['unit'].unique():\n",
                "        unit_data = df[df['unit'] == unit_id].sort_values('cycle')\n",
                "        if len(unit_data) < seq_len:\n",
                "            continue\n",
                "        features = unit_data[feature_cols].values\n",
                "        rul_values = unit_data['RUL'].values\n",
                "        for i in range(len(unit_data) - seq_len + 1):\n",
                "            sequences.append(features[i:i + seq_len])\n",
                "            targets.append(rul_values[i + seq_len - 1])\n",
                "    return np.array(sequences, dtype=np.float32), np.array(targets, dtype=np.float32)\n",
                "\n",
                "def create_test_sequences(df, seq_len, feature_cols):\n",
                "    sequences = []\n",
                "    for unit_id in sorted(df['unit'].unique()):\n",
                "        unit_data = df[df['unit'] == unit_id].tail(seq_len)\n",
                "        if len(unit_data) < seq_len:\n",
                "            pad_len = seq_len - len(unit_data)\n",
                "            padding = np.zeros((pad_len, len(feature_cols)))\n",
                "            seq = np.vstack([padding, unit_data[feature_cols].values])\n",
                "        else:\n",
                "            seq = unit_data[feature_cols].values\n",
                "        sequences.append(seq)\n",
                "    return np.array(sequences, dtype=np.float32)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CMAPSSDataset(Dataset):\n",
                "    def __init__(self, sequences, targets):\n",
                "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
                "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.sequences)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        return self.sequences[idx], self.targets[idx]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train All Subsets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_subset(subset):\n",
                "    print(f'\\n{\"=\"*60}')\n",
                "    print(f'ðŸš€ Training Transformer on {subset}')\n",
                "    print(f'{\"=\"*60}')\n",
                "    \n",
                "    # Load & preprocess\n",
                "    train_df, test_df, rul_true = load_data(subset)\n",
                "    train_df = compute_rul(train_df, MAX_RUL)\n",
                "    feature_cols = get_feature_columns()\n",
                "    \n",
                "    # Normalize\n",
                "    scaler = StandardScaler()\n",
                "    train_df[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
                "    test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
                "    \n",
                "    # Create sequences\n",
                "    X_train, y_train = create_sequences(train_df, SEQUENCE_LENGTH, feature_cols)\n",
                "    X_test = create_test_sequences(test_df, SEQUENCE_LENGTH, feature_cols)\n",
                "    print(f'Train: {len(X_train)}, Test: {len(X_test)}')\n",
                "    \n",
                "    # DataLoader\n",
                "    train_loader = DataLoader(CMAPSSDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
                "    \n",
                "    # Model\n",
                "    input_size = len(feature_cols)\n",
                "    model = TransformerRUL(\n",
                "        input_size=input_size,\n",
                "        d_model=D_MODEL,\n",
                "        n_heads=N_HEADS,\n",
                "        n_layers=N_LAYERS,\n",
                "        d_ff=D_FF,\n",
                "        dropout=DROPOUT\n",
                "    ).to(device)\n",
                "    \n",
                "    criterion = nn.MSELoss()\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
                "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
                "    \n",
                "    # Training\n",
                "    best_rmse = float('inf')\n",
                "    best_model = None\n",
                "    \n",
                "    for epoch in range(EPOCHS):\n",
                "        model.train()\n",
                "        total_loss = 0\n",
                "        \n",
                "        for seq, label in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}', leave=False):\n",
                "            seq, label = seq.to(device), label.to(device).unsqueeze(1)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            output = model(seq)\n",
                "            loss = criterion(output, label)\n",
                "            loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
                "            optimizer.step()\n",
                "            \n",
                "            total_loss += loss.item()\n",
                "        \n",
                "        scheduler.step()\n",
                "        \n",
                "        # Evaluate every 10 epochs\n",
                "        if (epoch + 1) % 10 == 0:\n",
                "            model.eval()\n",
                "            with torch.no_grad():\n",
                "                preds = model(torch.tensor(X_test).to(device)).cpu().numpy().flatten()\n",
                "            rmse = np.sqrt(np.mean((preds - rul_true) ** 2))\n",
                "            print(f'Epoch {epoch+1}/{EPOCHS}, Train Loss: {total_loss/len(train_loader):.4f}, Test RMSE: {rmse:.2f}')\n",
                "            \n",
                "            if rmse < best_rmse:\n",
                "                best_rmse = rmse\n",
                "                best_model = model.state_dict().copy()\n",
                "    \n",
                "    # Final evaluation with best model\n",
                "    model.load_state_dict(best_model)\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        predictions = model(torch.tensor(X_test).to(device)).cpu().numpy().flatten()\n",
                "    rmse = np.sqrt(np.mean((predictions - rul_true) ** 2))\n",
                "    print(f'\\nðŸ“Š Best Test RMSE: {rmse:.2f} cycles')\n",
                "    \n",
                "    # Save\n",
                "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
                "    save_file = f'{SAVE_PATH}/rul_transformer_{subset}.pth'\n",
                "    torch.save({\n",
                "        'model_state_dict': best_model,\n",
                "        'input_size': input_size,\n",
                "        'd_model': D_MODEL,\n",
                "        'n_heads': N_HEADS,\n",
                "        'n_layers': N_LAYERS,\n",
                "        'sequence_length': SEQUENCE_LENGTH,\n",
                "        'feature_cols': feature_cols,\n",
                "        'scaler_mean': scaler.mean_.tolist(),\n",
                "        'scaler_scale': scaler.scale_.tolist(),\n",
                "        'test_rmse': rmse\n",
                "    }, save_file)\n",
                "    print(f'âœ… Saved: {save_file}')\n",
                "    \n",
                "    return {'subset': subset, 'rmse': rmse, 'predictions': predictions, 'true': rul_true}\n",
                "\n",
                "# Train all\n",
                "all_results = []\n",
                "for subset in SUBSETS:\n",
                "    try:\n",
                "        results = train_subset(subset)\n",
                "        all_results.append(results)\n",
                "    except FileNotFoundError:\n",
                "        print(f'âš ï¸ {subset} not found, skipping...')\n",
                "\n",
                "print(f'\\n{\"=\"*60}')\n",
                "print('ðŸŽ‰ All training complete!')\n",
                "print(f'{\"=\"*60}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Results Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\nðŸ“‹ Final Results (Transformer):\\n')\n",
                "print(f'{\"Subset\":<10} {\"Test RMSE\":<15} {\"Status\":<15}')\n",
                "print('-' * 45)\n",
                "\n",
                "for r in all_results:\n",
                "    if r['rmse'] < 15:\n",
                "        status = 'ðŸ† Excellent'\n",
                "    elif r['rmse'] < 20:\n",
                "        status = 'âœ… Good'\n",
                "    elif r['rmse'] < 28:\n",
                "        status = 'âš ï¸ Acceptable'\n",
                "    else:\n",
                "        status = 'âŒ Needs work'\n",
                "    print(f'{r[\"subset\"]:<10} {r[\"rmse\"]:<15.2f} {status:<15}')\n",
                "\n",
                "avg_rmse = np.mean([r['rmse'] for r in all_results])\n",
                "print(f'\\nAverage RMSE: {avg_rmse:.2f} cycles')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, r in enumerate(all_results):\n",
                "    ax = axes[idx]\n",
                "    ax.scatter(r['true'], r['predictions'], alpha=0.5, s=20)\n",
                "    ax.plot([0, MAX_RUL], [0, MAX_RUL], 'r--', label='Perfect')\n",
                "    ax.set_xlabel('Actual RUL')\n",
                "    ax.set_ylabel('Predicted RUL')\n",
                "    ax.set_title(f'{r[\"subset\"]} (RMSE: {r[\"rmse\"]:.2f})')\n",
                "    ax.legend()\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('transformer_rul_results.png', dpi=150)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}