{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ‚öôÔ∏è Vibration Fault Classification Training (CWRU)\n",
                "\n",
                "This notebook trains a **1D CNN** on the CWRU Bearing dataset.\n",
                "\n",
                "**Features:**\n",
                "- Uses ALL .mat files (10 fault classes)\n",
                "- Can also use pre-processed .npz file\n",
                "- Proper 80/20 train/test split\n",
                "- Confusion matrix and per-class accuracy\n",
                "\n",
                "## Classes:\n",
                "| # | Class | Description |\n",
                "|---|-------|-------------|\n",
                "| 0 | Normal | Healthy bearing |\n",
                "| 1-3 | Ball_007/014/021 | Ball defect (0.007\"/0.014\"/0.021\") |\n",
                "| 4-6 | IR_007/014/021 | Inner race defect |\n",
                "| 7-9 | OR_007/014/021 | Outer race defect |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install scipy seaborn -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import numpy as np\n",
                "import scipy.io\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import glob\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============= UPDATE THIS PATH =============\n",
                "DATA_PATH = '/content/drive/MyDrive/MaintanenceAI/Data/CWRU'\n",
                "SAVE_PATH = '/content/drive/MyDrive/MaintanenceAI/trained_models'\n",
                "# =============================================\n",
                "\n",
                "# Data loading options\n",
                "USE_NPZ = True  # Set to True to use pre-processed .npz file (faster)\n",
                "\n",
                "# Hyperparameters\n",
                "WINDOW_SIZE = 2048\n",
                "NUM_CLASSES = 10\n",
                "EPOCHS = 30\n",
                "BATCH_SIZE = 64\n",
                "LEARNING_RATE = 1e-3\n",
                "TEST_SPLIT = 0.2\n",
                "\n",
                "print(f'Data path: {DATA_PATH}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class VibrationClassifier(nn.Module):\n",
                "    def __init__(self, num_classes=10):\n",
                "        super(VibrationClassifier, self).__init__()\n",
                "        self.features = nn.Sequential(\n",
                "            nn.Conv1d(1, 16, kernel_size=64, stride=2, padding=32),\n",
                "            nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2),\n",
                "            nn.Conv1d(16, 32, kernel_size=32, stride=2, padding=16),\n",
                "            nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),\n",
                "            nn.Conv1d(32, 64, kernel_size=16, stride=2, padding=8),\n",
                "            nn.BatchNorm1d(64), nn.ReLU(), nn.MaxPool1d(2),\n",
                "            nn.Conv1d(64, 128, kernel_size=8, stride=2, padding=4),\n",
                "            nn.BatchNorm1d(128), nn.ReLU(),\n",
                "            nn.AdaptiveAvgPool1d(1)\n",
                "        )\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(128, 64), nn.ReLU(),\n",
                "            nn.Linear(64, num_classes)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.features(x)\n",
                "        x = x.view(x.size(0), -1)\n",
                "        return self.classifier(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_from_npz(data_path):\n",
                "    \"\"\"Load pre-processed data from .npz file.\"\"\"\n",
                "    npz_path = os.path.join(data_path, 'CWRU_48k_load_1_CNN_data.npz')\n",
                "    data = np.load(npz_path)\n",
                "    return data['X'], data['y']\n",
                "\n",
                "def load_from_mat(data_path, window_size=2048):\n",
                "    \"\"\"Load from raw .mat files with proper segmentation.\"\"\"\n",
                "    raw_path = os.path.join(data_path, 'raw')\n",
                "    \n",
                "    file_patterns = {\n",
                "        'Normal': '*Normal*.mat',\n",
                "        'Ball_007': '*B007*.mat', 'Ball_014': '*B014*.mat', 'Ball_021': '*B021*.mat',\n",
                "        'IR_007': '*IR007*.mat', 'IR_014': '*IR014*.mat', 'IR_021': '*IR021*.mat',\n",
                "        'OR_007': '*OR007*.mat', 'OR_014': '*OR014*.mat', 'OR_021': '*OR021*.mat',\n",
                "    }\n",
                "    \n",
                "    all_segments, all_labels = [], []\n",
                "    \n",
                "    for label_name, pattern in file_patterns.items():\n",
                "        files = glob.glob(os.path.join(raw_path, pattern))\n",
                "        print(f'{label_name}: {len(files)} files found')\n",
                "        \n",
                "        for mat_file in files:\n",
                "            try:\n",
                "                mat = scipy.io.loadmat(mat_file)\n",
                "                for key in mat.keys():\n",
                "                    if 'DE_time' in key:\n",
                "                        signal = mat[key].flatten()\n",
                "                        n_segments = len(signal) // window_size\n",
                "                        for i in range(n_segments):\n",
                "                            segment = signal[i*window_size:(i+1)*window_size]\n",
                "                            all_segments.append(segment)\n",
                "                            all_labels.append(label_name)\n",
                "                        break\n",
                "            except Exception as e:\n",
                "                print(f'Error: {mat_file}: {e}')\n",
                "    \n",
                "    return np.array(all_segments), np.array(all_labels)\n",
                "\n",
                "# Load data\n",
                "print('Loading CWRU data...\\n')\n",
                "\n",
                "if USE_NPZ and os.path.exists(os.path.join(DATA_PATH, 'CWRU_48k_load_1_CNN_data.npz')):\n",
                "    print('Using pre-processed .npz file')\n",
                "    X, y = load_from_npz(DATA_PATH)\n",
                "    le = LabelEncoder()\n",
                "    # Create consistent label mapping\n",
                "    label_names = ['Normal', 'Ball_007', 'Ball_014', 'Ball_021', 'IR_007', 'IR_014', 'IR_021', 'OR_007', 'OR_014', 'OR_021']\n",
                "    le.fit(label_names)\n",
                "else:\n",
                "    print('Loading from raw .mat files')\n",
                "    X, y_str = load_from_mat(DATA_PATH, WINDOW_SIZE)\n",
                "    le = LabelEncoder()\n",
                "    y = le.fit_transform(y_str)\n",
                "\n",
                "print(f'\\nTotal samples: {len(X)}')\n",
                "print(f'Sample shape: {X.shape}')\n",
                "print(f'Classes: {le.classes_}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check class distribution\n",
                "unique, counts = np.unique(y, return_counts=True)\n",
                "print('\\nClass Distribution:')\n",
                "for cls, cnt in zip(unique, counts):\n",
                "    print(f'  Class {cls} ({le.classes_[cls]}): {cnt} samples')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=TEST_SPLIT, random_state=42, stratify=y\n",
                ")\n",
                "print(f'Train: {len(X_train)}, Test: {len(X_test)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset & DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CWRUDataset(Dataset):\n",
                "    def __init__(self, data, labels):\n",
                "        self.data = data\n",
                "        self.labels = labels\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        x = self.data[idx]\n",
                "        x = (x - np.mean(x)) / (np.std(x) + 1e-8)  # Normalize\n",
                "        return torch.tensor(x, dtype=torch.float32).unsqueeze(0), torch.tensor(self.labels[idx], dtype=torch.long)\n",
                "\n",
                "train_dataset = CWRUDataset(X_train, y_train)\n",
                "test_dataset = CWRUDataset(X_test, y_test)\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = VibrationClassifier(num_classes=NUM_CLASSES).to(device)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "\n",
                "train_losses, train_accs = [], []\n",
                "\n",
                "print('Starting Training...\\n')\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    total_loss, correct, total = 0, 0, 0\n",
                "    \n",
                "    for data, target in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}', leave=False):\n",
                "        data, target = data.to(device), target.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        output = model(data)\n",
                "        loss = criterion(output, target)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        pred = output.argmax(dim=1)\n",
                "        correct += pred.eq(target).sum().item()\n",
                "        total += target.size(0)\n",
                "    \n",
                "    avg_loss = total_loss / len(train_loader)\n",
                "    accuracy = 100. * correct / total\n",
                "    train_losses.append(avg_loss)\n",
                "    train_accs.append(accuracy)\n",
                "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
                "\n",
                "print('\\nTraining Complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "all_preds, all_targets = [], []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for data, target in test_loader:\n",
                "        data, target = data.to(device), target.to(device)\n",
                "        output = model(data)\n",
                "        pred = output.argmax(dim=1)\n",
                "        all_preds.extend(pred.cpu().numpy())\n",
                "        all_targets.extend(target.cpu().numpy())\n",
                "\n",
                "all_preds = np.array(all_preds)\n",
                "all_targets = np.array(all_targets)\n",
                "\n",
                "test_accuracy = 100. * np.mean(all_preds == all_targets)\n",
                "print(f'\\nüìä Test Accuracy: {test_accuracy:.2f}%')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classification Report\n",
                "print('\\nClassification Report:')\n",
                "print(classification_report(all_targets, all_preds, target_names=le.classes_))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(all_targets, all_preds)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
                "plt.title(f'Confusion Matrix (Accuracy: {test_accuracy:.1f}%)')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.tight_layout()\n",
                "plt.savefig('confusion_matrix.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(SAVE_PATH, exist_ok=True)\n",
                "save_file = os.path.join(SAVE_PATH, 'vibration_classifier.pth')\n",
                "\n",
                "torch.save({\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'label_encoder_classes': le.classes_.tolist(),\n",
                "    'test_accuracy': test_accuracy,\n",
                "    'num_classes': NUM_CLASSES\n",
                "}, save_file)\n",
                "\n",
                "print(f'‚úÖ Model saved: {save_file}')\n",
                "\n",
                "# Also save locally\n",
                "torch.save(model.state_dict(), 'vibration_classifier.pth')\n",
                "print('üì• Also saved locally: vibration_classifier.pth')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "axes[0].plot(train_losses)\n",
                "axes[0].set_title('Training Loss')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].grid(True)\n",
                "\n",
                "axes[1].plot(train_accs)\n",
                "axes[1].set_title('Training Accuracy')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy (%)')\n",
                "axes[1].grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_curves.png', dpi=150)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}