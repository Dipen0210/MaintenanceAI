{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOeWM0JFa04l"
      },
      "source": [
        "# üîä Audio Anomaly Detection v2 (Advanced Techniques)\n",
        "\n",
        "**Advanced improvements for better generalization:**\n",
        "- ‚úÖ Data augmentation (time stretch, pitch shift, noise, SpecAugment)\n",
        "- ‚úÖ Variational Autoencoder (VAE) with KL regularization\n",
        "- ‚úÖ Sub-cluster anomaly detection (k-means + distance)\n",
        "- ‚úÖ Mahalanobis distance scoring\n",
        "- ‚úÖ Segment-based processing (overlapping windows)\n",
        "- ‚úÖ Enhanced ensemble with weighted voting\n",
        "\n",
        "**Target:** AUC > 0.70 on both source and target domains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkQhGRKla04m",
        "outputId": "79d02bdf-b271-4e07-8658-ae90cadabdcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S-RB4o7a04n",
        "outputId": "766a4730-f513-425c-a17a-73fdeb3ca3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install panns-inference librosa tqdm scikit-learn joblib torch -q\n",
        "print('‚úÖ Installation complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK1_OAyVa04n",
        "outputId": "f6dfdeae-8e27-4272-c299-a96eef3f08e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñ•Ô∏è Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import librosa\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from panns_inference import AudioTagging\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
        "from sklearn.covariance import MinCovDet\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'üñ•Ô∏è Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OtDrBEoa04n"
      },
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeJ_tAsSa04n",
        "outputId": "1ac57e27-848d-47c5-b78f-eb1979ee7389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Config: ['fan', 'pump', 'valve']\n",
            "üìä Segment: 1.0s, Hop: 0.5s\n",
            "üìä PCA: 64 dims, Sub-clusters: 16\n"
          ]
        }
      ],
      "source": [
        "# ============= UPDATE THESE PATHS =============\n",
        "BASE_DATA_PATH = '/content/drive/MyDrive/Data'\n",
        "SAVE_PATH = '/content/drive/MyDrive/MaintanenceAI'\n",
        "# ===============================================\n",
        "\n",
        "MACHINE_TYPES = ['fan', 'pump', 'valve']\n",
        "SAMPLE_RATE = 16000  # Use 16kHz for efficiency\n",
        "SEGMENT_LENGTH = 1.0  # seconds per segment\n",
        "SEGMENT_HOP = 0.5  # 50% overlap\n",
        "N_MELS = 128\n",
        "N_MFCC = 20\n",
        "PCA_COMPONENTS = 64  # More aggressive reduction\n",
        "N_SUBCLUSTERS = 16  # For sub-cluster anomaly detection\n",
        "VAE_LATENT_DIM = 16\n",
        "VAE_EPOCHS = 100\n",
        "N_AUGMENTATIONS = 3  # Augmentation multiplier\n",
        "\n",
        "print(f'üéØ Config: {MACHINE_TYPES}')\n",
        "print(f'üìä Segment: {SEGMENT_LENGTH}s, Hop: {SEGMENT_HOP}s')\n",
        "print(f'üìä PCA: {PCA_COMPONENTS} dims, Sub-clusters: {N_SUBCLUSTERS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5rQlpu3a04n"
      },
      "source": [
        "## 2. Load PANNs Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWrt_iXFa04n",
        "outputId": "7577e45e-54ca-41d7-9101-beb5bcdd7d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading PANNs model...\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "Using CPU.\n",
            "‚úÖ PANNs model loaded!\n"
          ]
        }
      ],
      "source": [
        "print('üîÑ Loading PANNs model...')\n",
        "panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
        "print('‚úÖ PANNs model loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68s-1R8qa04o"
      },
      "source": [
        "## 3. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGSdqckja04o",
        "outputId": "dbb97e35-231c-4c06-970a-6d63e442a10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Augmentor ready!\n"
          ]
        }
      ],
      "source": [
        "class AudioAugmentor:\n",
        "    \"\"\"\n",
        "    Audio augmentation for robust training.\n",
        "    \"\"\"\n",
        "    def __init__(self, sr=SAMPLE_RATE):\n",
        "        self.sr = sr\n",
        "\n",
        "    def time_stretch(self, audio, rate=None):\n",
        "        \"\"\"Time stretch without changing pitch.\"\"\"\n",
        "        if rate is None:\n",
        "            rate = np.random.uniform(0.8, 1.2)\n",
        "        return librosa.effects.time_stretch(audio, rate=rate)\n",
        "\n",
        "    def pitch_shift(self, audio, n_steps=None):\n",
        "        \"\"\"Pitch shift.\"\"\"\n",
        "        if n_steps is None:\n",
        "            n_steps = np.random.uniform(-2, 2)\n",
        "        return librosa.effects.pitch_shift(audio, sr=self.sr, n_steps=n_steps)\n",
        "\n",
        "    def add_noise(self, audio, snr_db=None):\n",
        "        \"\"\"Add Gaussian noise.\"\"\"\n",
        "        if snr_db is None:\n",
        "            snr_db = np.random.uniform(20, 40)\n",
        "        signal_power = np.mean(audio**2)\n",
        "        noise_power = signal_power / (10**(snr_db/10))\n",
        "        noise = np.random.normal(0, np.sqrt(noise_power), len(audio))\n",
        "        return audio + noise\n",
        "\n",
        "    def time_mask(self, audio, max_mask_ratio=0.1):\n",
        "        \"\"\"Randomly mask time segments.\"\"\"\n",
        "        mask_len = int(len(audio) * np.random.uniform(0, max_mask_ratio))\n",
        "        start = np.random.randint(0, len(audio) - mask_len)\n",
        "        audio_masked = audio.copy()\n",
        "        audio_masked[start:start+mask_len] = 0\n",
        "        return audio_masked\n",
        "\n",
        "    def gain(self, audio, gain_db=None):\n",
        "        \"\"\"Apply gain.\"\"\"\n",
        "        if gain_db is None:\n",
        "            gain_db = np.random.uniform(-6, 6)\n",
        "        return audio * (10**(gain_db/20))\n",
        "\n",
        "    def augment(self, audio, n_augmentations=N_AUGMENTATIONS):\n",
        "        \"\"\"Apply random augmentations.\"\"\"\n",
        "        augmented = [audio]  # Original\n",
        "\n",
        "        for i in range(n_augmentations):\n",
        "            aug = audio.copy()\n",
        "            # Apply random subset of augmentations\n",
        "            if np.random.random() > 0.5:\n",
        "                aug = self.add_noise(aug)\n",
        "            if np.random.random() > 0.5:\n",
        "                aug = self.time_stretch(aug)\n",
        "            if np.random.random() > 0.7:\n",
        "                aug = self.pitch_shift(aug)\n",
        "            if np.random.random() > 0.7:\n",
        "                aug = self.time_mask(aug)\n",
        "            if np.random.random() > 0.5:\n",
        "                aug = self.gain(aug)\n",
        "            augmented.append(aug)\n",
        "\n",
        "        return augmented\n",
        "\n",
        "augmentor = AudioAugmentor()\n",
        "print('‚úÖ Augmentor ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSZptbVqa04o"
      },
      "source": [
        "## 4. Segment-Based Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VAS1_xda04o",
        "outputId": "bf693ccc-f24d-447e-e5a8-9369352f76f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Feature extraction ready!\n",
            "üìä Segment feature dimension: 477\n"
          ]
        }
      ],
      "source": [
        "def extract_log_mel_spectrogram(audio, sr, n_mels=N_MELS, n_fft=1024, hop_length=512):\n",
        "    \"\"\"Extract log mel spectrogram.\"\"\"\n",
        "    mel = librosa.feature.melspectrogram(\n",
        "        y=audio, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length\n",
        "    )\n",
        "    log_mel = librosa.power_to_db(mel, ref=np.max)\n",
        "    return log_mel\n",
        "\n",
        "\n",
        "def extract_segment_features(audio, sr):\n",
        "    \"\"\"\n",
        "    Extract comprehensive features from an audio segment.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "\n",
        "    # Log-mel spectrogram statistics\n",
        "    log_mel = extract_log_mel_spectrogram(audio, sr)\n",
        "    features.extend([\n",
        "        np.mean(log_mel, axis=1),  # Mean per mel band\n",
        "        np.std(log_mel, axis=1),   # Std per mel band\n",
        "        np.max(log_mel, axis=1) - np.min(log_mel, axis=1),  # Range\n",
        "    ])\n",
        "\n",
        "    # MFCCs with deltas\n",
        "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "    features.extend([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.std(mfcc, axis=1),\n",
        "        np.mean(mfcc_delta, axis=1),\n",
        "        np.mean(mfcc_delta2, axis=1),\n",
        "    ])\n",
        "\n",
        "    # Spectral features\n",
        "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
        "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]\n",
        "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
        "    spectral_flatness = librosa.feature.spectral_flatness(y=audio)[0]\n",
        "    zcr = librosa.feature.zero_crossing_rate(audio)[0]\n",
        "    rms = librosa.feature.rms(y=audio)[0]\n",
        "\n",
        "    features.extend([\n",
        "        [np.mean(spectral_centroid), np.std(spectral_centroid)],\n",
        "        [np.mean(spectral_bandwidth), np.std(spectral_bandwidth)],\n",
        "        [np.mean(spectral_rolloff), np.std(spectral_rolloff)],\n",
        "        [np.mean(spectral_flatness), np.std(spectral_flatness)],\n",
        "        [np.mean(zcr), np.std(zcr)],\n",
        "        [np.mean(rms), np.std(rms), np.max(rms)],\n",
        "    ])\n",
        "\n",
        "    # Flatten and concatenate\n",
        "    return np.concatenate([np.array(f).flatten() for f in features])\n",
        "\n",
        "\n",
        "def extract_file_features(file_path, augment=False):\n",
        "    \"\"\"\n",
        "    Extract features from an audio file using segment-based processing.\n",
        "    Returns multiple feature vectors (one per segment).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, mono=True)\n",
        "\n",
        "        # Apply augmentation if training\n",
        "        if augment:\n",
        "            audio_versions = augmentor.augment(audio)\n",
        "        else:\n",
        "            audio_versions = [audio]\n",
        "\n",
        "        all_features = []\n",
        "        segment_samples = int(SEGMENT_LENGTH * sr)\n",
        "        hop_samples = int(SEGMENT_HOP * sr)\n",
        "\n",
        "        for audio_ver in audio_versions:\n",
        "            # Pad if too short\n",
        "            if len(audio_ver) < segment_samples:\n",
        "                audio_ver = np.pad(audio_ver, (0, segment_samples - len(audio_ver)))\n",
        "\n",
        "            # Extract segments\n",
        "            for start in range(0, len(audio_ver) - segment_samples + 1, hop_samples):\n",
        "                segment = audio_ver[start:start + segment_samples]\n",
        "                feat = extract_segment_features(segment, sr)\n",
        "                all_features.append(feat)\n",
        "\n",
        "        return np.array(all_features)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error: {file_path}: {e}')\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_panns_embedding(file_path, model):\n",
        "    \"\"\"Extract PANNs embedding (full file).\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(file_path, sr=32000, mono=True)\n",
        "        if len(audio) < 32000:\n",
        "            audio = np.pad(audio, (0, 32000 - len(audio)))\n",
        "        audio = audio[np.newaxis, :]\n",
        "        _, embedding = model.inference(audio)\n",
        "        return embedding[0]\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_hybrid_features(file_paths, panns_model, augment=False, desc='Extracting'):\n",
        "    \"\"\"\n",
        "    Extract segment-based features for all files.\n",
        "    \"\"\"\n",
        "    all_segment_features = []\n",
        "    file_indices = []  # Track which segments belong to which file\n",
        "    valid_paths = []\n",
        "\n",
        "    for idx, path in enumerate(tqdm(file_paths, desc=desc)):\n",
        "        # Segment features\n",
        "        seg_feat = extract_file_features(path, augment=augment)\n",
        "        if seg_feat is None or len(seg_feat) == 0:\n",
        "            continue\n",
        "\n",
        "        all_segment_features.append(seg_feat)\n",
        "        file_indices.append(np.full(len(seg_feat), idx))\n",
        "        valid_paths.append(path)\n",
        "\n",
        "    # Concatenate all segments\n",
        "    segment_features = np.vstack(all_segment_features)\n",
        "    file_indices = np.concatenate(file_indices)\n",
        "\n",
        "    return segment_features, file_indices, valid_paths\n",
        "\n",
        "\n",
        "print('‚úÖ Feature extraction ready!')\n",
        "# Test feature dimension\n",
        "test_audio = np.random.randn(int(SEGMENT_LENGTH * SAMPLE_RATE))\n",
        "test_feat = extract_segment_features(test_audio, SAMPLE_RATE)\n",
        "print(f'üìä Segment feature dimension: {len(test_feat)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4V7_88Da04o"
      },
      "source": [
        "## 5. Variational Autoencoder (VAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbUoTPtza04o",
        "outputId": "224fd67d-cc19-4350-cc0f-951f8b77f028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ VAE ready!\n"
          ]
        }
      ],
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    VAE for anomaly detection with KL regularization.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, latent_dim=VAE_LATENT_DIM):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        # Latent space\n",
        "        self.fc_mu = nn.Linear(64, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, input_dim),\n",
        "        )\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        return self.fc_mu(h), self.fc_logvar(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon = self.decode(z)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "    def get_anomaly_score(self, x):\n",
        "        \"\"\"Get anomaly score (reconstruction error + KL divergence).\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            recon, mu, logvar = self.forward(x)\n",
        "            # Reconstruction error\n",
        "            recon_loss = torch.mean((x - recon)**2, dim=1)\n",
        "            # KL divergence\n",
        "            kl_loss = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp(), dim=1)\n",
        "            # Combined score\n",
        "            score = recon_loss + 0.1 * kl_loss\n",
        "        return score.cpu().numpy()\n",
        "\n",
        "\n",
        "def train_vae(X_train, epochs=VAE_EPOCHS, batch_size=128, lr=1e-3):\n",
        "    \"\"\"Train VAE on normal data.\"\"\"\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = VariationalAutoencoder(input_dim).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "\n",
        "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
        "    dataset = torch.utils.data.TensorDataset(X_tensor)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            x = batch[0]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            recon, mu, logvar = model(x)\n",
        "\n",
        "            # VAE loss\n",
        "            recon_loss = F.mse_loss(recon, x, reduction='mean')\n",
        "            kl_loss = -0.5 * torch.mean(1 + logvar - mu**2 - logvar.exp())\n",
        "            loss = recon_loss + 0.1 * kl_loss\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'    Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "print('‚úÖ VAE ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBVe374ma04o"
      },
      "source": [
        "## 6. Sub-Cluster Anomaly Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2wHRT8la04o",
        "outputId": "15a889f0-66e6-4951-f35e-344680fff2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Sub-cluster detector ready!\n"
          ]
        }
      ],
      "source": [
        "class SubClusterDetector:\n",
        "    \"\"\"\n",
        "    Anomaly detection using sub-clusters of normal data.\n",
        "    Normal data may have multiple modes; we model each with a cluster.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters=N_SUBCLUSTERS):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        self.cluster_covs = []\n",
        "        self.cluster_means = []\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Fit sub-clusters on normal data.\"\"\"\n",
        "        self.kmeans.fit(X)\n",
        "        labels = self.kmeans.labels_\n",
        "\n",
        "        self.cluster_means = []\n",
        "        self.cluster_covs = []\n",
        "\n",
        "        for i in range(self.n_clusters):\n",
        "            cluster_data = X[labels == i]\n",
        "            if len(cluster_data) > 1:\n",
        "                self.cluster_means.append(np.mean(cluster_data, axis=0))\n",
        "                try:\n",
        "                    cov = MinCovDet().fit(cluster_data).covariance_\n",
        "                except:\n",
        "                    cov = np.cov(cluster_data.T) + 1e-6 * np.eye(cluster_data.shape[1])\n",
        "                self.cluster_covs.append(cov)\n",
        "\n",
        "        self.fitted = True\n",
        "\n",
        "    def score_samples(self, X):\n",
        "        \"\"\"\n",
        "        Score samples: minimum distance to any cluster.\n",
        "        Higher score = more anomalous.\n",
        "        \"\"\"\n",
        "        scores = []\n",
        "        for x in X:\n",
        "            min_dist = float('inf')\n",
        "            for mean, cov in zip(self.cluster_means, self.cluster_covs):\n",
        "                try:\n",
        "                    cov_inv = np.linalg.pinv(cov)\n",
        "                    dist = mahalanobis(x, mean, cov_inv)\n",
        "                except:\n",
        "                    dist = np.linalg.norm(x - mean)\n",
        "                min_dist = min(min_dist, dist)\n",
        "            scores.append(min_dist)\n",
        "        return np.array(scores)\n",
        "\n",
        "print('‚úÖ Sub-cluster detector ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1tMoC1Ka04p"
      },
      "source": [
        "## 7. Mahalanobis Distance Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AExt1cPRa04p",
        "outputId": "1b5eb208-35ec-4dee-8231-33bc773b125f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mahalanobis detector ready!\n"
          ]
        }
      ],
      "source": [
        "class MahalanobisDetector:\n",
        "    \"\"\"\n",
        "    Anomaly detection using Mahalanobis distance.\n",
        "    \"\"\"\n",
        "    def __init__(self, robust=True):\n",
        "        self.robust = robust\n",
        "        self.mean = None\n",
        "        self.cov_inv = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Fit on normal data.\"\"\"\n",
        "        self.mean = np.mean(X, axis=0)\n",
        "\n",
        "        if self.robust:\n",
        "            try:\n",
        "                mcd = MinCovDet().fit(X)\n",
        "                cov = mcd.covariance_\n",
        "            except:\n",
        "                cov = np.cov(X.T)\n",
        "        else:\n",
        "            cov = np.cov(X.T)\n",
        "\n",
        "        # Add regularization for stability\n",
        "        cov = cov + 1e-6 * np.eye(cov.shape[0])\n",
        "        self.cov_inv = np.linalg.pinv(cov)\n",
        "\n",
        "    def score_samples(self, X):\n",
        "        \"\"\"Mahalanobis distance from normal distribution.\"\"\"\n",
        "        scores = []\n",
        "        for x in X:\n",
        "            scores.append(mahalanobis(x, self.mean, self.cov_inv))\n",
        "        return np.array(scores)\n",
        "\n",
        "print('‚úÖ Mahalanobis detector ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6iseQKSa04p"
      },
      "source": [
        "## 8. Advanced Ensemble Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egt4NVbra04p",
        "outputId": "519dab6d-864d-43e6-99fa-bf709e252220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Advanced ensemble ready!\n"
          ]
        }
      ],
      "source": [
        "class AdvancedEnsembleDetector:\n",
        "    \"\"\"\n",
        "    Advanced ensemble with multiple complementary methods.\n",
        "    \"\"\"\n",
        "    def __init__(self, pca_components=PCA_COMPONENTS):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.pca = PCA(n_components=pca_components)\n",
        "\n",
        "        # Detectors\n",
        "        self.gmm = GaussianMixture(\n",
        "            n_components=8, covariance_type='full',\n",
        "            random_state=42, max_iter=300, reg_covar=1e-5\n",
        "        )\n",
        "        self.iforest = IsolationForest(\n",
        "            n_estimators=200, contamination=0.05,\n",
        "            random_state=42, n_jobs=-1\n",
        "        )\n",
        "        self.lof = LocalOutlierFactor(\n",
        "            n_neighbors=30, contamination=0.05,\n",
        "            novelty=True, n_jobs=-1\n",
        "        )\n",
        "        self.subcluster = SubClusterDetector(n_clusters=N_SUBCLUSTERS)\n",
        "        self.mahal = MahalanobisDetector(robust=True)\n",
        "        self.vae = None\n",
        "\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X_train):\n",
        "        \"\"\"Fit all detectors.\"\"\"\n",
        "        print('  üìä Preprocessing...')\n",
        "        X_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_pca = self.pca.fit_transform(X_scaled)\n",
        "        print(f'    PCA variance: {sum(self.pca.explained_variance_ratio_)*100:.1f}%')\n",
        "\n",
        "        print('  üîß Training detectors...')\n",
        "        self.gmm.fit(X_pca)\n",
        "        self.iforest.fit(X_pca)\n",
        "        self.lof.fit(X_pca)\n",
        "\n",
        "        print('  üéØ Training sub-cluster detector...')\n",
        "        self.subcluster.fit(X_pca)\n",
        "\n",
        "        print('  üìè Training Mahalanobis detector...')\n",
        "        self.mahal.fit(X_pca)\n",
        "\n",
        "        print('  üß† Training VAE...')\n",
        "        self.vae = train_vae(X_pca)\n",
        "\n",
        "        self.fitted = True\n",
        "        print('  ‚úÖ Ensemble training complete!')\n",
        "\n",
        "    def _get_individual_scores(self, X_pca):\n",
        "        \"\"\"Get scores from all methods.\"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        # GMM\n",
        "        scores['gmm'] = -self.gmm.score_samples(X_pca)\n",
        "\n",
        "        # Isolation Forest\n",
        "        scores['iforest'] = -self.iforest.score_samples(X_pca)\n",
        "\n",
        "        # LOF\n",
        "        scores['lof'] = -self.lof.score_samples(X_pca)\n",
        "\n",
        "        # Sub-cluster\n",
        "        scores['subcluster'] = self.subcluster.score_samples(X_pca)\n",
        "\n",
        "        # Mahalanobis\n",
        "        scores['mahal'] = self.mahal.score_samples(X_pca)\n",
        "\n",
        "        # VAE\n",
        "        X_tensor = torch.FloatTensor(X_pca).to(device)\n",
        "        scores['vae'] = self.vae.get_anomaly_score(X_tensor)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def score_files(self, all_segment_features, file_indices):\n",
        "        \"\"\"\n",
        "        Score multiple files.\n",
        "        \"\"\"\n",
        "        X_scaled = self.scaler.transform(all_segment_features)\n",
        "        X_pca = self.pca.transform(X_scaled)\n",
        "\n",
        "        # Get segment-level scores\n",
        "        segment_scores = self._get_individual_scores(X_pca)\n",
        "\n",
        "        # Aggregate by file\n",
        "        unique_files = np.unique(file_indices)\n",
        "        file_scores = {name: [] for name in segment_scores}\n",
        "\n",
        "        for file_idx in unique_files:\n",
        "            mask = file_indices == file_idx\n",
        "            for name, scores in segment_scores.items():\n",
        "                # Use multiple aggregations\n",
        "                max_score = np.max(scores[mask])\n",
        "                mean_score = np.mean(scores[mask])\n",
        "                p90_score = np.percentile(scores[mask], 90)\n",
        "                # Weighted combination\n",
        "                agg_score = 0.5 * max_score + 0.3 * p90_score + 0.2 * mean_score\n",
        "                file_scores[name].append(agg_score)\n",
        "\n",
        "        return {name: np.array(scores) for name, scores in file_scores.items()}\n",
        "\n",
        "    def ensemble_score(self, all_segment_features, file_indices):\n",
        "        \"\"\"\n",
        "        Get ensemble score per file.\n",
        "        \"\"\"\n",
        "        individual_scores = self.score_files(all_segment_features, file_indices)\n",
        "\n",
        "        # Normalize each\n",
        "        normalized = {}\n",
        "        for name, scores in individual_scores.items():\n",
        "            s_min, s_max = scores.min(), scores.max()\n",
        "            if s_max > s_min:\n",
        "                normalized[name] = (scores - s_min) / (s_max - s_min)\n",
        "            else:\n",
        "                normalized[name] = np.zeros_like(scores)\n",
        "\n",
        "        # Weighted ensemble\n",
        "        weights = {\n",
        "            'gmm': 1.0,\n",
        "            'iforest': 1.0,\n",
        "            'lof': 0.8,\n",
        "            'subcluster': 1.2,\n",
        "            'mahal': 1.0,\n",
        "            'vae': 1.2\n",
        "        }\n",
        "\n",
        "        ensemble = sum(weights[k] * normalized[k] for k in weights)\n",
        "        ensemble /= sum(weights.values())\n",
        "\n",
        "        return ensemble, individual_scores\n",
        "\n",
        "print('‚úÖ Advanced ensemble ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG4rJkuYa04p"
      },
      "source": [
        "## 9. Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZwpDRyNa04p",
        "outputId": "a8c15bdf-91b2-4ca9-e85c-f66038d53ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training functions ready!\n"
          ]
        }
      ],
      "source": [
        "def parse_filename(filename):\n",
        "    \"\"\"Parse DCASE filename.\"\"\"\n",
        "    basename = os.path.basename(filename)\n",
        "    section_match = re.search(r'section_(\\d+)', basename)\n",
        "    section = section_match.group(1) if section_match else 'unknown'\n",
        "    label = 'anomaly' if 'anomaly' in basename else 'normal'\n",
        "    return {'section': section, 'label': label}\n",
        "\n",
        "\n",
        "def group_by_section(file_paths):\n",
        "    \"\"\"Group files by section.\"\"\"\n",
        "    sections = defaultdict(list)\n",
        "    for path in file_paths:\n",
        "        info = parse_filename(path)\n",
        "        sections[info['section']].append(path)\n",
        "    return dict(sections)\n",
        "\n",
        "\n",
        "def get_label(filename):\n",
        "    return 1 if 'anomaly' in filename else 0\n",
        "\n",
        "\n",
        "def evaluate_detector(detector, test_segment_features, file_indices, y_test, test_name):\n",
        "    \"\"\"Evaluate detector on test data.\"\"\"\n",
        "    ensemble_scores, individual_scores = detector.ensemble_score(test_segment_features, file_indices)\n",
        "\n",
        "    results = {}\n",
        "    for name, scores in individual_scores.items():\n",
        "        try:\n",
        "            auc = roc_auc_score(y_test, scores)\n",
        "        except:\n",
        "            auc = 0.5\n",
        "        results[f'{name}_auc'] = auc\n",
        "\n",
        "    try:\n",
        "        ensemble_auc = roc_auc_score(y_test, ensemble_scores)\n",
        "    except:\n",
        "        ensemble_auc = 0.5\n",
        "    results['ensemble_auc'] = ensemble_auc\n",
        "\n",
        "    # Best method\n",
        "    all_aucs = {k: v for k, v in results.items() if k.endswith('_auc')}\n",
        "    best_key = max(all_aucs, key=all_aucs.get)\n",
        "    best_auc = all_aucs[best_key]\n",
        "    best_method = best_key.replace('_auc', '')\n",
        "\n",
        "    # Get best scores for accuracy\n",
        "    if best_method == 'ensemble':\n",
        "        best_scores = ensemble_scores\n",
        "    else:\n",
        "        best_scores = individual_scores[best_method]\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, best_scores)\n",
        "    best_idx = np.argmax(tpr - fpr)\n",
        "    predictions = (best_scores > thresholds[best_idx]).astype(int)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    results['best_auc'] = best_auc\n",
        "    results['best_method'] = best_method\n",
        "    results['accuracy'] = accuracy\n",
        "\n",
        "    status = '‚úÖ' if best_auc > 0.7 else '‚ö†Ô∏è' if best_auc > 0.6 else '‚ùå'\n",
        "\n",
        "    print(f'  {test_name}:')\n",
        "    print(f'    GMM={results[\"gmm_auc\"]:.3f}, IF={results[\"iforest_auc\"]:.3f}, '\n",
        "          f'LOF={results[\"lof_auc\"]:.3f}, SC={results[\"subcluster_auc\"]:.3f}, '\n",
        "          f'MH={results[\"mahal_auc\"]:.3f}, VAE={results[\"vae_auc\"]:.3f}')\n",
        "    print(f'    Ensemble={ensemble_auc:.3f} ‚Üí Best: {best_auc:.3f} ({best_method}) Acc: {accuracy*100:.1f}% {status}')\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def train_section(machine_type, section_id, train_files, source_test, target_test):\n",
        "    \"\"\"Train model for a specific section.\"\"\"\n",
        "    print(f'\\n  üìÇ Section {section_id}: {len(train_files)} files')\n",
        "\n",
        "    # Extract features with augmentation\n",
        "    print('  üéµ Extracting training features (with augmentation)...')\n",
        "    train_seg_feat, train_idx, _ = extract_hybrid_features(\n",
        "        train_files, panns_model, augment=True, desc=f'Train sec{section_id}'\n",
        "    )\n",
        "    print(f'    Segment features: {train_seg_feat.shape}')\n",
        "\n",
        "    # Train detector\n",
        "    detector = AdvancedEnsembleDetector()\n",
        "    detector.fit(train_seg_feat)\n",
        "\n",
        "    # Filter test files by section\n",
        "    source_section = [f for f in source_test if f'section_{section_id}' in f]\n",
        "    target_section = [f for f in target_test if f'section_{section_id}' in f]\n",
        "\n",
        "    results = {'section': section_id}\n",
        "\n",
        "    # Evaluate source\n",
        "    if source_section:\n",
        "        print(f'\\n  üìä Evaluating source_test...')\n",
        "        src_seg, src_idx, src_paths = extract_hybrid_features(\n",
        "            source_section, panns_model, augment=False, desc=f'SrcTest sec{section_id}'\n",
        "        )\n",
        "        y_source = np.array([get_label(os.path.basename(p)) for p in src_paths])\n",
        "        # Remap file indices to 0..n-1\n",
        "        unique_idx = np.unique(src_idx)\n",
        "        idx_map = {old: new for new, old in enumerate(unique_idx)}\n",
        "        src_idx_mapped = np.array([idx_map[i] for i in src_idx])\n",
        "        results['source'] = evaluate_detector(detector, src_seg, src_idx_mapped, y_source, 'source_test')\n",
        "\n",
        "    # Evaluate target\n",
        "    if target_section:\n",
        "        print(f'\\n  üìä Evaluating target_test...')\n",
        "        tgt_seg, tgt_idx, tgt_paths = extract_hybrid_features(\n",
        "            target_section, panns_model, augment=False, desc=f'TgtTest sec{section_id}'\n",
        "        )\n",
        "        y_target = np.array([get_label(os.path.basename(p)) for p in tgt_paths])\n",
        "        unique_idx = np.unique(tgt_idx)\n",
        "        idx_map = {old: new for new, old in enumerate(unique_idx)}\n",
        "        tgt_idx_mapped = np.array([idx_map[i] for i in tgt_idx])\n",
        "        results['target'] = evaluate_detector(detector, tgt_seg, tgt_idx_mapped, y_target, 'target_test')\n",
        "\n",
        "    return detector, results\n",
        "\n",
        "\n",
        "def train_machine(machine_type):\n",
        "    \"\"\"Train all sections for a machine type.\"\"\"\n",
        "    print(f'\\n{\"=\"*70}')\n",
        "    print(f'üîä {machine_type.upper()} - Advanced Training v2')\n",
        "    print(f'{\"=\"*70}')\n",
        "\n",
        "    data_path = os.path.join(BASE_DATA_PATH, machine_type)\n",
        "\n",
        "    train_files = sorted(glob.glob(os.path.join(data_path, 'train', '*.wav')))\n",
        "    source_test = sorted(glob.glob(os.path.join(data_path, 'source_test', '*.wav')))\n",
        "    target_test = sorted(glob.glob(os.path.join(data_path, 'target_test', '*.wav')))\n",
        "\n",
        "    # Keep only normal training files\n",
        "    train_files = [f for f in train_files if 'normal' in os.path.basename(f)]\n",
        "\n",
        "    print(f'üìÅ Train: {len(train_files)}, Source: {len(source_test)}, Target: {len(target_test)}')\n",
        "\n",
        "    if not train_files:\n",
        "        return None\n",
        "\n",
        "    sections = group_by_section(train_files)\n",
        "    print(f'üìÇ Sections: {list(sections.keys())}')\n",
        "\n",
        "    all_results = {}\n",
        "    for section_id in sorted(sections.keys()):\n",
        "        detector, results = train_section(\n",
        "            machine_type, section_id, sections[section_id],\n",
        "            source_test, target_test\n",
        "        )\n",
        "        all_results[section_id] = results\n",
        "\n",
        "    # Save\n",
        "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "    save_file = os.path.join(SAVE_PATH, f'audio_advanced_v2_{machine_type}.pkl')\n",
        "    joblib.dump({'results': all_results, 'sections': list(sections.keys())}, save_file)\n",
        "    print(f'\\n‚úÖ Saved: {save_file}')\n",
        "\n",
        "    return all_results\n",
        "\n",
        "print('‚úÖ Training functions ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV69gfraa04p"
      },
      "source": [
        "## 10. Train All Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx0hc6vZa04p",
        "outputId": "e35eafed-7c31-4d4e-c390-0be374a395f9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üîä FAN - Advanced Training v2\n",
            "======================================================================\n",
            "üìÅ Train: 3009, Source: 600, Target: 600\n",
            "üìÇ Sections: ['00', '01', '02']\n",
            "\n",
            "  üìÇ Section 00: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [19:21<00:00,  1.16s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Segment features: (75970, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 94.1%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9531\n",
            "    Epoch 40/100, Loss: 0.9092\n",
            "    Epoch 60/100, Loss: 0.8867\n",
            "    Epoch 80/100, Loss: 0.8725\n",
            "    Epoch 100/100, Loss: 0.8636\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SrcTest sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:01<00:00,  1.65it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  source_test:\n",
            "    GMM=0.591, IF=0.511, LOF=0.642, SC=0.608, MH=0.517, VAE=0.517\n",
            "    Ensemble=0.564 ‚Üí Best: 0.642 (lof) Acc: 64.5% ‚ö†Ô∏è\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TgtTest sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [02:00<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  target_test:\n",
            "    GMM=0.645, IF=0.563, LOF=0.584, SC=0.610, MH=0.605, VAE=0.554\n",
            "    Ensemble=0.603 ‚Üí Best: 0.645 (gmm) Acc: 64.0% ‚ö†Ô∏è\n",
            "\n",
            "  üìÇ Section 01: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [18:07<00:00,  1.08s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Segment features: (75953, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 94.2%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9521\n",
            "    Epoch 40/100, Loss: 0.9055\n",
            "    Epoch 60/100, Loss: 0.8809\n",
            "    Epoch 80/100, Loss: 0.8642\n",
            "    Epoch 100/100, Loss: 0.8614\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SrcTest sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:59<00:00,  1.67it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  source_test:\n",
            "    GMM=0.571, IF=0.542, LOF=0.588, SC=0.595, MH=0.531, VAE=0.537\n",
            "    Ensemble=0.557 ‚Üí Best: 0.595 (subcluster) Acc: 60.0% ‚ùå\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TgtTest sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:55<00:00,  1.73it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  target_test:\n",
            "    GMM=0.501, IF=0.523, LOF=0.471, SC=0.527, MH=0.551, VAE=0.502\n",
            "    Ensemble=0.519 ‚Üí Best: 0.551 (mahal) Acc: 56.0% ‚ùå\n",
            "\n",
            "  üìÇ Section 02: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [18:01<00:00,  1.08s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Segment features: (75762, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 94.3%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9593\n",
            "    Epoch 40/100, Loss: 0.9025\n",
            "    Epoch 60/100, Loss: 0.8763\n",
            "    Epoch 80/100, Loss: 0.8683\n",
            "    Epoch 100/100, Loss: 0.8586\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SrcTest sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:53<00:00,  1.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  source_test:\n",
            "    GMM=0.602, IF=0.561, LOF=0.665, SC=0.628, MH=0.553, VAE=0.562\n",
            "    Ensemble=0.608 ‚Üí Best: 0.665 (lof) Acc: 63.5% ‚ö†Ô∏è\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TgtTest sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:52<00:00,  1.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  target_test:\n",
            "    GMM=0.492, IF=0.493, LOF=0.550, SC=0.490, MH=0.462, VAE=0.489\n",
            "    Ensemble=0.493 ‚Üí Best: 0.550 (lof) Acc: 57.5% ‚ùå\n",
            "\n",
            "‚úÖ Saved: /content/drive/MyDrive/MaintanenceAI/audio_advanced_v2_fan.pkl\n",
            "\n",
            "======================================================================\n",
            "üîä PUMP - Advanced Training v2\n",
            "======================================================================\n",
            "üìÅ Train: 3009, Source: 600, Target: 600\n",
            "üìÇ Sections: ['00', '01', '02']\n",
            "\n",
            "  üìÇ Section 00: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [18:54<00:00,  1.13s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Segment features: (75718, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 93.8%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9717\n",
            "    Epoch 40/100, Loss: 0.9199\n",
            "    Epoch 60/100, Loss: 0.8946\n",
            "    Epoch 80/100, Loss: 0.8786\n",
            "    Epoch 100/100, Loss: 0.8756\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SrcTest sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:51<00:00,  1.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  source_test:\n",
            "    GMM=0.621, IF=0.569, LOF=0.636, SC=0.635, MH=0.503, VAE=0.604\n",
            "    Ensemble=0.600 ‚Üí Best: 0.636 (lof) Acc: 62.5% ‚ö†Ô∏è\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TgtTest sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:53<00:00,  1.77it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  target_test:\n",
            "    GMM=0.523, IF=0.517, LOF=0.486, SC=0.516, MH=0.594, VAE=0.514\n",
            "    Ensemble=0.531 ‚Üí Best: 0.594 (mahal) Acc: 57.0% ‚ùå\n",
            "\n",
            "  üìÇ Section 01: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [18:08<00:00,  1.08s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Segment features: (75926, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 93.7%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9728\n",
            "    Epoch 40/100, Loss: 0.9316\n",
            "    Epoch 60/100, Loss: 0.9045\n",
            "    Epoch 80/100, Loss: 0.8813\n",
            "    Epoch 100/100, Loss: 0.8817\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SrcTest sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:55<00:00,  1.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  source_test:\n",
            "    GMM=0.613, IF=0.524, LOF=0.624, SC=0.580, MH=0.536, VAE=0.533\n",
            "    Ensemble=0.576 ‚Üí Best: 0.624 (lof) Acc: 62.5% ‚ö†Ô∏è\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TgtTest sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:55<00:00,  1.73it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  target_test:\n",
            "    GMM=0.456, IF=0.443, LOF=0.480, SC=0.459, MH=0.499, VAE=0.442\n",
            "    Ensemble=0.454 ‚Üí Best: 0.499 (mahal) Acc: 56.0% ‚ùå\n",
            "\n",
            "  üìÇ Section 02: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [17:52<00:00,  1.07s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Segment features: (75728, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 93.8%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9666\n",
            "    Epoch 40/100, Loss: 0.9108\n",
            "    Epoch 60/100, Loss: 0.8893\n",
            "    Epoch 80/100, Loss: 0.8751\n",
            "    Epoch 100/100, Loss: 0.8750\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "SrcTest sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:56<00:00,  1.71it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  source_test:\n",
            "    GMM=0.625, IF=0.530, LOF=0.636, SC=0.610, MH=0.495, VAE=0.568\n",
            "    Ensemble=0.578 ‚Üí Best: 0.636 (lof) Acc: 62.0% ‚ö†Ô∏è\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TgtTest sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:53<00:00,  1.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  target_test:\n",
            "    GMM=0.559, IF=0.488, LOF=0.589, SC=0.564, MH=0.491, VAE=0.518\n",
            "    Ensemble=0.530 ‚Üí Best: 0.589 (lof) Acc: 59.5% ‚ùå\n",
            "\n",
            "‚úÖ Saved: /content/drive/MyDrive/MaintanenceAI/audio_advanced_v2_pump.pkl\n",
            "\n",
            "======================================================================\n",
            "üîä VALVE - Advanced Training v2\n",
            "======================================================================\n",
            "üìÅ Train: 3009, Source: 600, Target: 600\n",
            "üìÇ Sections: ['00', '01', '02']\n",
            "\n",
            "  üìÇ Section 00: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [18:35<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Segment features: (75979, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 93.4%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9627\n",
            "    Epoch 40/100, Loss: 0.9140\n",
            "    Epoch 60/100, Loss: 0.8849\n",
            "    Epoch 80/100, Loss: 0.8703\n",
            "    Epoch 100/100, Loss: 0.8633\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SrcTest sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:53<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  source_test:\n",
            "    GMM=0.468, IF=0.365, LOF=0.487, SC=0.477, MH=0.245, VAE=0.380\n",
            "    Ensemble=0.358 ‚Üí Best: 0.487 (lof) Acc: 52.5% ‚ùå\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TgtTest sec00: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:51<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  target_test:\n",
            "    GMM=0.444, IF=0.389, LOF=0.526, SC=0.502, MH=0.296, VAE=0.396\n",
            "    Ensemble=0.397 ‚Üí Best: 0.526 (lof) Acc: 56.0% ‚ùå\n",
            "\n",
            "  üìÇ Section 01: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [17:50<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Segment features: (75846, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 93.3%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9868\n",
            "    Epoch 40/100, Loss: 0.9398\n",
            "    Epoch 60/100, Loss: 0.9121\n",
            "    Epoch 80/100, Loss: 0.8981\n",
            "    Epoch 100/100, Loss: 0.8961\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SrcTest sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:54<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  source_test:\n",
            "    GMM=0.521, IF=0.451, LOF=0.598, SC=0.475, MH=0.441, VAE=0.494\n",
            "    Ensemble=0.480 ‚Üí Best: 0.598 (lof) Acc: 61.5% ‚ùå\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TgtTest sec01: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:52<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  target_test:\n",
            "    GMM=0.483, IF=0.414, LOF=0.483, SC=0.502, MH=0.508, VAE=0.377\n",
            "    Ensemble=0.464 ‚Üí Best: 0.508 (mahal) Acc: 55.0% ‚ùå\n",
            "\n",
            "  üìÇ Section 02: 1003 files\n",
            "  üéµ Extracting training features (with augmentation)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1003/1003 [18:04<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Segment features: (75864, 477)\n",
            "  üìä Preprocessing...\n",
            "    PCA variance: 94.1%\n",
            "  üîß Training detectors...\n",
            "  üéØ Training sub-cluster detector...\n",
            "  üìè Training Mahalanobis detector...\n",
            "  üß† Training VAE...\n",
            "    Epoch 20/100, Loss: 0.9343\n",
            "    Epoch 40/100, Loss: 0.8895\n",
            "    Epoch 60/100, Loss: 0.8590\n",
            "    Epoch 80/100, Loss: 0.8457\n",
            "    Epoch 100/100, Loss: 0.8429\n",
            "  ‚úÖ Ensemble training complete!\n",
            "\n",
            "  üìä Evaluating source_test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SrcTest sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:53<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  source_test:\n",
            "    GMM=0.647, IF=0.547, LOF=0.658, SC=0.597, MH=0.543, VAE=0.535\n",
            "    Ensemble=0.600 ‚Üí Best: 0.658 (lof) Acc: 61.5% ‚ö†Ô∏è\n",
            "\n",
            "  üìä Evaluating target_test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TgtTest sec02: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:53<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  target_test:\n",
            "    GMM=0.467, IF=0.435, LOF=0.470, SC=0.489, MH=0.472, VAE=0.424\n",
            "    Ensemble=0.470 ‚Üí Best: 0.489 (subcluster) Acc: 52.5% ‚ùå\n",
            "\n",
            "‚úÖ Saved: /content/drive/MyDrive/MaintanenceAI/audio_advanced_v2_valve.pkl\n",
            "\n",
            "======================================================================\n",
            "üéâ Training Complete!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "all_machine_results = {}\n",
        "\n",
        "for machine in MACHINE_TYPES:\n",
        "    results = train_machine(machine)\n",
        "    if results:\n",
        "        all_machine_results[machine] = results\n",
        "\n",
        "print(f'\\n{\"=\"*70}')\n",
        "print('üéâ Training Complete!')\n",
        "print(f'{\"=\"*70}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "208ORmHGa04p"
      },
      "source": [
        "## 11. Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1AejN8xa04p",
        "outputId": "720eb92f-ab22-468b-be76-34cb8e164ad1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìã Final Results Summary:\n",
            "\n",
            "Machine  Sec   Test     GMM    IF     LOF    SC     MH     VAE    Ens    Best   Acc   \n",
            "-----------------------------------------------------------------------------------------------\n",
            "fan      00    source   0.591  0.511  0.642  0.608  0.517  0.517  0.564  0.642  64.5 %\n",
            "fan      00    target   0.645  0.563  0.584  0.610  0.605  0.554  0.603  0.645  64.0 %\n",
            "fan      01    source   0.571  0.542  0.588  0.595  0.531  0.537  0.557  0.595  60.0 %\n",
            "fan      01    target   0.501  0.523  0.471  0.527  0.551  0.502  0.519  0.551  56.0 %\n",
            "fan      02    source   0.602  0.561  0.665  0.628  0.553  0.562  0.608  0.665  63.5 %\n",
            "fan      02    target   0.492  0.493  0.550  0.490  0.462  0.489  0.493  0.550  57.5 %\n",
            "pump     00    source   0.621  0.569  0.636  0.635  0.503  0.604  0.600  0.636  62.5 %\n",
            "pump     00    target   0.523  0.517  0.486  0.516  0.594  0.514  0.531  0.594  57.0 %\n",
            "pump     01    source   0.613  0.524  0.624  0.580  0.536  0.533  0.576  0.624  62.5 %\n",
            "pump     01    target   0.456  0.443  0.480  0.459  0.499  0.442  0.454  0.499  56.0 %\n",
            "pump     02    source   0.625  0.530  0.636  0.610  0.495  0.568  0.578  0.636  62.0 %\n",
            "pump     02    target   0.559  0.488  0.589  0.564  0.491  0.518  0.530  0.589  59.5 %\n",
            "valve    00    source   0.468  0.365  0.487  0.477  0.245  0.380  0.358  0.487  52.5 %\n",
            "valve    00    target   0.444  0.389  0.526  0.502  0.296  0.396  0.397  0.526  56.0 %\n",
            "valve    01    source   0.521  0.451  0.598  0.475  0.441  0.494  0.480  0.598  61.5 %\n",
            "valve    01    target   0.483  0.414  0.483  0.502  0.508  0.377  0.464  0.508  55.0 %\n",
            "valve    02    source   0.647  0.547  0.658  0.597  0.543  0.535  0.600  0.658  61.5 %\n",
            "valve    02    target   0.467  0.435  0.470  0.489  0.472  0.424  0.470  0.489  52.5 %\n",
            "\n",
            "üéØ Average AUC: 0.5830\n",
            "üìä Range: 0.4872 - 0.6652\n",
            "\n",
            "‚úÖ Good (>0.7): 0, ‚ö†Ô∏è Moderate (0.6-0.7): 7, ‚ùå Poor (<0.6): 11\n"
          ]
        }
      ],
      "source": [
        "print('\\nüìã Final Results Summary:\\n')\n",
        "print(f'{\"Machine\":<8} {\"Sec\":<5} {\"Test\":<8} {\"GMM\":<6} {\"IF\":<6} {\"LOF\":<6} {\"SC\":<6} {\"MH\":<6} {\"VAE\":<6} {\"Ens\":<6} {\"Best\":<6} {\"Acc\":<6}')\n",
        "print('-' * 95)\n",
        "\n",
        "all_aucs = []\n",
        "for machine, sec_results in all_machine_results.items():\n",
        "    for sec_id, results in sec_results.items():\n",
        "        for test_type in ['source', 'target']:\n",
        "            if test_type in results:\n",
        "                m = results[test_type]\n",
        "                all_aucs.append(m['best_auc'])\n",
        "                print(\n",
        "                    f'{machine:<8} {sec_id:<5} {test_type:<8} '\n",
        "                    f'{m[\"gmm_auc\"]:<6.3f} {m[\"iforest_auc\"]:<6.3f} {m[\"lof_auc\"]:<6.3f} '\n",
        "                    f'{m[\"subcluster_auc\"]:<6.3f} {m[\"mahal_auc\"]:<6.3f} {m[\"vae_auc\"]:<6.3f} '\n",
        "                    f'{m[\"ensemble_auc\"]:<6.3f} {m[\"best_auc\"]:<6.3f} {m[\"accuracy\"]*100:<5.1f}%'\n",
        "                )\n",
        "\n",
        "print(f'\\nüéØ Average AUC: {np.mean(all_aucs):.4f}')\n",
        "print(f'üìä Range: {np.min(all_aucs):.4f} - {np.max(all_aucs):.4f}')\n",
        "\n",
        "# Count successes\n",
        "good = sum(1 for a in all_aucs if a > 0.7)\n",
        "moderate = sum(1 for a in all_aucs if 0.6 < a <= 0.7)\n",
        "poor = sum(1 for a in all_aucs if a <= 0.6)\n",
        "print(f'\\n‚úÖ Good (>0.7): {good}, ‚ö†Ô∏è Moderate (0.6-0.7): {moderate}, ‚ùå Poor (<0.6): {poor}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}